# typed: false

# DO NOT EDIT MANUALLY
# This is an autogenerated file for types exported from the `benchmark-ips` gem.
# Please instead update this file by running `bin/tapioca gem benchmark-ips`.

# Performance benchmarking library
module Benchmark
  extend ::Benchmark::Compare
  extend ::Benchmark::IPS
end

# Functionality of performaing comparison between reports.
#
# Usage:
#
# Add +x.compare!+ to perform comparison between reports.
#
# Example:
#   > Benchmark.ips do |x|
#     x.report('Reduce using tag')     { [*1..10].reduce(:+) }
#     x.report('Reduce using to_proc') { [*1..10].reduce(&:+) }
#     x.compare!
#   end
#
#   Calculating -------------------------------------
#       Reduce using tag     19216 i/100ms
#   Reduce using to_proc     17437 i/100ms
#   -------------------------------------------------
#       Reduce using tag   278950.0 (±8.5%) i/s -    1402768 in   5.065112s
#   Reduce using to_proc   247295.4 (±8.0%) i/s -    1238027 in   5.037299s
#
#   Comparison:
#       Reduce using tag:   278950.0 i/s
#   Reduce using to_proc:   247295.4 i/s - 1.13x slower
#
# Besides regular Calculating report, this will also indicates which one is slower.
#
# +x.compare!+ also takes an +order: :baseline+ option.
#
# Example:
#  > Benchmark.ips do |x|
#   x.report('Reduce using block')   { [*1..10].reduce { |sum, n| sum + n } }
#   x.report('Reduce using tag')     { [*1..10].reduce(:+) }
#   x.report('Reduce using to_proc') { [*1..10].reduce(&:+) }
#   x.compare!(order: :baseline)
# end
#
# Calculating -------------------------------------
#   Reduce using block    886.202k (± 2.2%) i/s -      4.521M in   5.103774s
#     Reduce using tag      1.821M (± 1.6%) i/s -      9.111M in   5.004183s
# Reduce using to_proc    895.948k (± 1.6%) i/s -      4.528M in   5.055368s
#
# Comparison:
#   Reduce using block:   886202.5 i/s
#     Reduce using tag:  1821055.0 i/s - 2.05x  (± 0.00) faster
# Reduce using to_proc:   895948.1 i/s - same-ish: difference falls within error
#
# The first report is considered the baseline against which other reports are compared.
module Benchmark::Compare
  # Compare between reports, prints out facts of each report:
  # runtime, comparative speed difference.
  #
  # @param entries [Array<Report::Entry>] Reports to compare.
  def compare(*entries, order: T.unsafe(nil)); end
end

# Benchmark in iterations per second, no more guessing!
#
# @see https://github.com/evanphx/benchmark-ips
module Benchmark::IPS
  # Measure code in block, each code's benchmarked result will display in
  # iteration per second with standard deviation in given time.
  #
  # @param time [Integer] Specify how long should benchmark your code in seconds.
  # @param warmup [Integer] Specify how long should Warmup time run in seconds.
  # @return [Report]
  # @yield [job]
  def ips(*args); end

  class << self
    # Set options for running the benchmarks.
    # :format => [:human, :raw]
    #    :human format narrows precision and scales results for readability
    #    :raw format displays 6 places of precision and exact iteration counts
    def options; end
  end
end

# CODENAME of current version.
Benchmark::IPS::CODENAME = T.let(T.unsafe(nil), String)

module Benchmark::IPS::Helpers
  private

  def scale(value); end

  class << self
    def scale(value); end
  end
end

# Benchmark jobs.
class Benchmark::IPS::Job
  # Instantiate the Benchmark::IPS::Job.
  #
  # @return [Job] a new instance of Job
  def initialize(opts = T.unsafe(nil)); end

  # @return [Boolean]
  def all_results_have_been_run?; end

  def clear_held_results; end

  # Determining whether to run comparison utility.
  #
  # @return [Boolean] true if needs to run compare.
  def compare; end

  # Run comparison utility.
  def compare!(order: T.unsafe(nil)); end

  # Return true if job needs to be compared.
  #
  # @return [Boolean] Need to compare?
  def compare?; end

  # Confidence.
  #
  # @return [Integer]
  def confidence; end

  # Confidence.
  #
  # @return [Integer]
  def confidence=(_arg0); end

  # Job configuration options, set +@warmup+ and +@time+.
  #
  # @option opts
  # @option opts
  # @option iterations
  # @param opts [Hash] a customizable set of options
  # @param iterations [Hash] a customizable set of options
  def config(opts); end

  # Create report by add entry to +@full_report+.
  #
  # @param label [String] Report item label.
  # @param measured_us [Integer] Measured time in microsecond.
  # @param iter [Integer] Iterations.
  # @param samples [Array<Float>] Sampled iterations per second.
  # @param cycles [Integer] Number of Cycles.
  # @return [Report::Entry] Entry with data.
  def create_report(label, measured_us, iter, samples, cycles); end

  def create_stats(samples); end

  # Calculate the cycles needed to run for approx 100ms,
  # given the number of iterations to run the given time.
  #
  # @param time_msec [Float] Each iteration's time in ms.
  # @param iters [Integer] Iterations.
  # @return [Integer] Cycles per 100ms.
  def cycles_per_100ms(time_msec, iters); end

  # Report object containing information about the run.
  #
  # @return [Report] the report object.
  def full_report; end

  # Generate json from +@full_report+.
  def generate_json; end

  # Determining whether to hold results between Ruby invocations
  #
  # @return [Boolean]
  def hold; end

  # Hold after each iteration.
  #
  # @param held_path [String] File name to store hold file.
  def hold!(held_path); end

  # Determining whether to hold results between Ruby invocations
  #
  # @return [Boolean]
  def hold=(_arg0); end

  # Return true if results are held while multiple Ruby invocations
  #
  # @return [Boolean] Need to hold results between multiple Ruby invocations?
  def hold?; end

  # Registers the given label and block pair in the job list.
  #
  # @param label [String] Label of benchmarked code.
  # @param str [String] Code to be benchmarked.
  # @param blk [Proc] Code to be benchmarked.
  # @raise [ArgumentError] Raises if str and blk are both present.
  # @raise [ArgumentError] Raises if str and blk are both absent.
  def item(label = T.unsafe(nil), str = T.unsafe(nil), &blk); end

  # Warmup and calculation iterations.
  #
  # @return [Integer]
  def iterations; end

  # Warmup and calculation iterations.
  #
  # @return [Integer]
  def iterations=(_arg0); end

  # Calculate the interations per second given the number
  # of cycles run and the time in microseconds that elapsed.
  #
  # @param cycles [Integer] Cycles.
  # @param time_us [Integer] Time in microsecond.
  # @return [Float] Iteration per second.
  def iterations_per_sec(cycles, time_us); end

  # Generate json to given path, defaults to "data.json".
  def json!(path = T.unsafe(nil)); end

  # Return true if job needs to generate json.
  #
  # @return [Boolean] Need to generate json?
  def json?; end

  # Two-element arrays, consisting of label and block pairs.
  #
  # @return [Array<Entry>] list of entries
  def list; end

  def load_held_results; end

  # Silence output
  #
  # @return [Boolean]
  def quiet; end

  def quiet=(val); end

  # Registers the given label and block pair in the job list.
  #
  # @param label [String] Label of benchmarked code.
  # @param str [String] Code to be benchmarked.
  # @param blk [Proc] Code to be benchmarked.
  # @raise [ArgumentError] Raises if str and blk are both present.
  # @raise [ArgumentError] Raises if str and blk are both absent.
  def report(label = T.unsafe(nil), str = T.unsafe(nil), &blk); end

  def reporter(quiet:); end
  def run; end

  # Run calculation.
  def run_benchmark; end

  # Run comparison of entries in +@full_report+.
  def run_comparison; end

  # Return true if items are to be run one at a time.
  # For the traditional hold, this is true
  #
  # @return [Boolean] Run just a single item?
  def run_single?; end

  # Run warmup.
  def run_warmup; end

  # Save interim results. Similar to hold, but all reports are run
  # The report label must change for each invocation.
  # One way to achieve this is to include the version in the label.
  #
  # @param held_path [String] File name to store hold file.
  def save!(held_path); end

  def save_held_results; end

  # Statistics model.
  #
  # @return [Object]
  def stats; end

  # Statistics model.
  #
  # @return [Object]
  def stats=(_arg0); end

  # Suite
  #
  # @return [Benchmark::IPS::NoopSuite]
  def suite; end

  def suite=(suite); end

  # Calculation time setter and getter (in seconds).
  #
  # @return [Integer]
  def time; end

  # Calculation time setter and getter (in seconds).
  #
  # @return [Integer]
  def time=(_arg0); end

  # Calculate the time difference of before and after in microseconds.
  #
  # @param before [Time] time.
  # @param after [Time] time.
  # @return [Float] Time difference of before and after.
  def time_us(before, after); end

  # Storing Iterations in time period.
  #
  # @return [Hash]
  def timing; end

  # Warmup time setter and getter (in seconds).
  #
  # @return [Integer]
  def warmup; end

  # Warmup time setter and getter (in seconds).
  #
  # @return [Integer]
  def warmup=(_arg0); end
end

# Entries in Benchmark Jobs.
class Benchmark::IPS::Job::Entry
  # Instantiate the Benchmark::IPS::Job::Entry.
  #
  # @param label [#to_s] Label of Benchmarked code.
  # @param action [String, Proc] Code to be benchmarked.
  # @raise [ArgumentError] Raises when action is not String or not responding to +call+.
  # @return [Entry] a new instance of Entry
  def initialize(label, action); end

  # The benchmarking action.
  #
  # @return [String, Proc] Code to be called, could be String / Proc.
  def action; end

  # Call action by given times.
  #
  # @param times [Integer] Times to call +@action+.
  # @return [Integer] Number of times the +@action+ has been called.
  def call_times(times); end

  def compile_block; end
  def compile_block_with_manual_loop; end

  # Compile code into +call_times+ method.
  #
  # @param str [String] Code to be compiled.
  # @return [Symbol] :call_times.
  def compile_string(str); end

  # The label of benchmarking action.
  #
  # @return [#to_s] Label of action.
  def label; end
end

# The percentage of the expected runtime to allow
# before reporting a weird runtime
Benchmark::IPS::Job::MAX_TIME_SKEW = T.let(T.unsafe(nil), Float)

# Microseconds per 100 millisecond.
Benchmark::IPS::Job::MICROSECONDS_PER_100MS = T.let(T.unsafe(nil), Integer)

# Microseconds per second.
Benchmark::IPS::Job::MICROSECONDS_PER_SECOND = T.let(T.unsafe(nil), Integer)

class Benchmark::IPS::Job::NoopReport
  def add_report(a, b); end
  def footer; end
  def running(a, b); end
  def start_running; end
  def start_warming; end
  def warming(a, b); end
  def warmup_stats(a, b); end
end

Benchmark::IPS::Job::POW_2_30 = T.let(T.unsafe(nil), Integer)

class Benchmark::IPS::Job::StdoutReport
  # @return [StdoutReport] a new instance of StdoutReport
  def initialize; end

  def add_report(item, caller); end
  def footer; end
  def running(label, _warmup); end
  def start_running; end
  def start_warming; end
  def warming(label, _warmup); end
  def warmup_stats(_warmup_time_us, timing); end

  private

  # @return [Symbol] format used for benchmarking
  def format; end

  # Add padding to label's right if label's length < 20,
  # Otherwise add a new line and 20 whitespaces.
  #
  # @return [String] Right justified label.
  def rjust(label); end
end

class Benchmark::IPS::NoopSuite
  def add_report(a, b); end
  def footer; end
  def running(a, b); end
  def start_running; end
  def start_warming; end
  def warming(a, b); end
  def warmup_stats(a, b); end
end

# Report contains benchmarking entries.
# Perform operations like add new entry, run comparison between entries.
class Benchmark::IPS::Report
  # Instantiate the Report.
  #
  # @return [Report] a new instance of Report
  def initialize; end

  # Add entry to report.
  #
  # @param label [String] Entry label.
  # @param microseconds [Integer] Measured time in microsecond.
  # @param iters [Integer] Iterations.
  # @param stats [Object] Statistical results.
  # @param measurement_cycle [Integer] Number of cycles.
  # @return [Report::Entry] Last added entry.
  def add_entry(label, microseconds, iters, stats, measurement_cycle); end

  # Entries data in array for generate json.
  # Each entry is a hash, consists of:
  #   name:   Entry#label
  #   ips:    Entry#ips
  #   stddev: Entry#ips_sd
  #   microseconds: Entry#microseconds
  #   iterations:   Entry#iterations
  #   cycles:       Entry#measurement_cycles
  #
  # @return [Array<Hash<Symbol,String|Float|Integer>] Array of hashes] Array<Hash<Symbol,String|Float|Integer>] Array of hashes
  def data; end

  # Entry to represent each benchmarked code in Report.
  #
  # @return [Array<Report::Entry>] Entries in Report.
  def entries; end

  # Generate json from Report#data to given path.
  #
  # @param path [String] path to generate json.
  def generate_json(path); end

  # Run comparison of entries.
  def run_comparison(order); end
end

# Represents benchmarking code data for Report.
class Benchmark::IPS::Report::Entry
  # Instantiate the Benchmark::IPS::Report::Entry.
  #
  # @param label [#to_s] Label of entry.
  # @param us [Integer] Measured time in microsecond.
  # @param iters [Integer] Iterations.
  # @param stats [Object] Statistics.
  # @param cycles [Integer] Number of Cycles.
  # @return [Entry] a new instance of Entry
  def initialize(label, us, iters, stats, cycles); end

  # Return Entry body text with left padding.
  # Body text contains information of iteration per second with
  # percentage of standard deviation, iterations in runtime.
  #
  # @return [String] Left justified body.
  def body; end

  # Print entry to current standard output ($stdout).
  def display; end

  # Return entry's standard deviation of iteration per second in percentage.
  #
  # @return [Float] +@ips_sd+ in percentage.
  def error_percentage; end

  # Return header with padding if +@label+ is < length of 20.
  #
  # @return [String] Right justified header (+@label+).
  def header; end

  # LEGACY: Iterations per second.
  #
  # @return [Float] number of iterations per second.
  def ips; end

  # LEGACY: Standard deviation of iteration per second.
  #
  # @return [Float] standard deviation of iteration per second.
  def ips_sd; end

  # Number of Iterations.
  #
  # @return [Integer] number of iterations.
  def iterations; end

  # Label of entry.
  #
  # @return [String] the label of entry.
  def label; end

  # Number of Cycles.
  #
  # @return [Integer] number of cycles.
  def measurement_cycle; end

  # Measured time in microsecond.
  #
  # @return [Integer] number of microseconds.
  def microseconds; end

  # Return entry's microseconds in seconds.
  #
  # @return [Float] +@microseconds+ in seconds.
  def runtime; end

  def samples; end

  # Return entry's microseconds in seconds.
  #
  # @return [Float] +@microseconds+ in seconds.
  def seconds; end

  # Control if the total time the job took is reported.
  # Typically this value is not significant because it's very
  # close to the expected time, so it's supressed by default.
  def show_total_time!; end

  # Statistical summary of samples.
  #
  # @return [Object] statisical summary.
  def stats; end

  # Return string repesentation of Entry object.
  #
  # @return [String] Header and body.
  def to_s; end
end

module Benchmark::IPS::Stats; end

class Benchmark::IPS::Stats::Bootstrap
  include ::Benchmark::IPS::Stats::StatsMetric

  # @return [Bootstrap] a new instance of Bootstrap
  def initialize(samples, confidence); end

  # Average stat value
  #
  # @return [Float] central_tendency
  def central_tendency; end

  # Returns the value of attribute data.
  def data; end

  def dependencies; end

  # Returns the value of attribute error.
  def error; end

  def footer; end

  # Returns the value of attribute samples.
  def samples; end

  # Determines how much slower this stat is than the baseline stat
  # if this average is lower than the faster baseline, higher average is better (e.g. ips) (calculate accordingly)
  #
  # @param baseline [SD|Bootstrap] faster baseline
  def slowdown(baseline); end

  def speedup(baseline); end
end

class Benchmark::IPS::Stats::SD
  include ::Benchmark::IPS::Stats::StatsMetric

  # @return [SD] a new instance of SD
  def initialize(samples); end

  # Average stat value
  #
  # @return [Float] central_tendency
  def central_tendency; end

  # Returns the value of attribute error.
  def error; end

  def footer; end

  # Returns the value of attribute samples.
  def samples; end

  # Determines how much slower this stat is than the baseline stat
  # if this average is lower than the faster baseline, higher average is better (e.g. ips) (calculate accordingly)
  #
  # @param baseline [SD|Bootstrap] faster baseline
  def slowdown(baseline); end

  def speedup(baseline); end
end

module Benchmark::IPS::Stats::StatsMetric
  # Return entry's standard deviation of iteration per second in percentage.
  #
  # @return [Float] +@ips_sd+ in percentage.
  def error_percentage; end

  # @return [Boolean]
  def overlaps?(baseline); end
end

# Benchmark-ips Gem version.
Benchmark::IPS::VERSION = T.let(T.unsafe(nil), String)

# Perform caclulations on Timing results.
module Benchmark::Timing
  class << self
    def add_second(t, s); end

    # Recycle used objects by starting Garbage Collector.
    def clean_env; end

    # Calculate (arithmetic) mean of given samples.
    #
    # @param samples [Array] Samples to calculate mean.
    # @return [Float] Mean of given samples.
    def mean(samples); end

    def now; end

    # Calculate standard deviation of given samples.
    #
    # @param samples [Array] Samples to calculate standard deviation.
    # @param m [Float] Optional mean (Expected value).
    # @return [Float] standard deviation of given samples.
    def stddev(samples, m = T.unsafe(nil)); end

    def time_us(before, after); end

    # Calculate variance of given samples.
    #
    # @param m [Float] Optional mean (Expected value).
    # @return [Float] Variance of given samples.
    def variance(samples, m = T.unsafe(nil)); end
  end
end

# Microseconds per second.
Benchmark::Timing::MICROSECONDS_PER_SECOND = T.let(T.unsafe(nil), Integer)
